\documentclass[red]{beamer}

\usepackage{mathtools}
\makeatletter
\newcommand{\explain}[2]{\underset{\mathclap{\overset{\uparrow}{#2}}}{#1}}
\newcommand{\explainup}[2]{\overset{\mathclap{\underset{\downarrow}{#2}}}{#1}}
\makeatother


\usetheme{Antibes}
\usecolortheme{lily}

%\usepackage{gb4e}

%\usepackage{color}

\usepackage{fancyvrb}
\usepackage{xcolor}

%\fvset{frame=single,framesep=1mm,fontfamily=courier,fontsize=\scriptsize,numbers=left,framerule=.3mm,numbersep=1mm,commandchars=\\\{\}}



\usepackage[english]{babel}
%\usepackage[latin1]{inputenc}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{epstopdf}

\usepackage{mathptmx}
\usepackage{amsthm}

\usepackage{esint}

\usepackage{graphicx}




\setbeamercovered{transparent}


\title[lecture 3: MCMC sampling, model evaluation]{Statistical methods for linguistic research: Advanced Tools}
\author{Shravan Vasishth}
\institute{Department of Linguistics\\
University of Potsdam, Germany}

\setbeamerfont{page number in head/foot}{size=\large}
\setbeamertemplate{footline}[frame number]


\begin{document}

<<setup,include=FALSE,cache=FALSE>>=
library(knitr)
library(coda)

# set global chunk options, put figures into folder
options(replace.assign=TRUE,show.signif.stars=FALSE)
opts_chunk$set(fig.path='figures/figure-', fig.align='center', fig.show='hold')
options(replace.assign=TRUE,width=75)
opts_chunk$set(dev='postscript')
@

\frame{\titlepage}

\section{Introduction}

\begin{frame}\frametitle{Goals for this lecture}

In this lecture I plan to discuss

\begin{enumerate}
\item Markov Chain Monte Carlo (MCMC)  Sampling
\item Model evaluation and checking model assumptions:
\begin{enumerate}
\item Using lmer.
\item Using Stan or JAGS.
\item The Box-Cox procedure for stabilizing variance.
\end{enumerate}
\end{enumerate}

\end{frame}


\section{Markov Chain Monte Carlo sampling}

\begin{frame}[fragile]\frametitle{Monte Carlo integration}

It sounds fancy, but basically this amounts to sampling from a distribution, and computing summaries like the mean. 

Formally, we calculate E(f(X)) by drawing samples $\{X_1,\dots,X_n\}$ and then approximating:

\begin{equation}
E(f(X))\approx \frac{1}{n}\sum f(X)
\end{equation}

For example: 

<<>>=
x<-rnorm(1000,mean=0,sd=1)
mean(x)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Monte Carlo integration}

We can increase sample size to as large as we want.

We can also compute quantities like $P(X<1.96)$ by sampling:

<<>>=
mean(x<1.96)
@

\end{frame}


\begin{frame}\frametitle{MCMC sampling}

\begin{enumerate}
\item
So, we can compute summary statistics using simulation if we know the distribution we are sampling from. 
\item
However, if we only know up to proportionality the form of the distribution to sample from, how do we get these samples to summarize from?  Recall:

Posterior $\propto$ Lik Prior

\item
Markov Chain Monte Carlo  (MCMC) methods provide that capability: they allow you to sample from distributions you only know up to proportionality.
\end{enumerate}

\end{frame}

\begin{frame}[fragile]\frametitle{Markov chain sampling}

We have been doing non-Markov chain sampling:

<<>>=
indep.samp<-rnorm(500,mean=0,sd=1)
head(indep.samp)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Markov chain sampling}

The vector of values sampled here are statistically independent. 

<<echo=FALSE,fig.height=4>>=
plot(1:500,indep.samp,type="l")
@

\end{frame}


\begin{frame}[fragile]\frametitle{Markov chain}

If the current value influences the next one, we have a Markov chain.
Here is a simple Markov chain: the $i$-th draw is dependent on the $i-1$-th draw:

<<markovchainexample>>=
nsim<-500
x<-rep(NA,nsim)
y<-rep(NA,nsim)
x[1]<-rnorm(1) ## initialize x
for(i in 2:nsim){
## draw i-th value based on i-1-th value:  
y[i]<-rnorm(1,mean=x[i-1],sd=1)
x[i]<-y[i]
}
@

\end{frame}


\begin{frame}[fragile]\frametitle{Markov chain}

<<echo=FALSE,fig.height=4>>=
plot(1:nsim,y,type="l")
@

\end{frame}

\begin{frame}\frametitle{MCMC sampling}

The basic idea of MCMC is that we sample from an approximate distribution, and then correct or adjust the values.

\end{frame}


\subsection{Two simple sampling methods}


\begin{frame}[fragile]\frametitle{Two sampling methods}

Next, I will discuss two sampling methods, in order to give a flavor of how we can sample from a distribution.

\begin{enumerate}
\item
Inverse sampling
\item
Rejection sampling
\end{enumerate}

The problem we have to solve is: given a distribution, say

$f(x) = \frac{1}{40} (2x + 3)$

how can we sample from it? 

Recall that with the normal distribution it's easy: use \texttt{rnorm}.
\end{frame}

%xxxx

\begin{frame}[fragile]\frametitle{Inverse sampling}

This method works when we can know the closed form of the pdf we want to simulate from and can derive the inverse of that function.

Steps:

\begin{enumerate}
\item Sample one number $u$ from $Unif(0,1)$. Let $u=F(z)=\int_L^z f(x)\, dx $ (here, $L$ is the lower bound of the pdf f).
\item Then $z=F^{-1}(u)$ is a draw from $f(x)$.
\end{enumerate}

\end{frame}

\begin{frame}[fragile]\frametitle{Inverse sampling}

Example:
Let $f(x) = \frac{1}{40} (2x + 3)$, with $0<x<5$. 
<<echo=FALSE,fig.height=3>>=
fn<-function(x){(1/40) * (2*x + 3)}
x<-seq(0,5,by=0.01)
plot(x,fn(x),type="l")
@
\end{frame}

\begin{frame}[fragile]\frametitle{Inverse sampling}

\begin{enumerate}
\item
Let $f(x) = \frac{1}{40} (2x + 3)$, with $0<x<5$.
\item
We have to draw a number from the uniform distribution and then solve for z, which amounts to finding the inverse function:

\begin{equation}
u = \int_0^z \frac{1}{40} (2x + 3)\, dx
\end{equation}
\item
Solving the integral:

\begin{equation}
\begin{split}
\int_0^z \frac{1}{40} (2x + 3) =& \frac{1}{40} \int_0^z ( 2x + 3) \,dx \\
=& \frac{1}{40}\left[ \frac{2x^2}{2} + 3x
\right]_0^z=
 \frac{1}{40}[z^2+3z]
\end{split}
\end{equation}

\end{enumerate}

\end{frame}


\begin{frame}[fragile]\frametitle{Inverse sampling}

So, if $u = \frac{1}{40} [z^2 + 3z]$, then,
$40u = z^2 + 3z$.

Completing the square, we can express $z$ in terms of $u$: 

Notice that 

\begin{equation}
(z + \frac{3}{2})^2 = z^2 + 3z + \frac{9}{4}
\end{equation}

Therefore 

\begin{equation}
40 u   + \frac{9}{4} = (z + \frac{3}{2})^2
\end{equation}

Solving for z we get:

\begin{equation}
z = \frac{1}{2}( \sqrt{160u + 9}-3)
\end{equation}

Note: you will never need to do this yourself, this is just intended to give you a sense of how sampling works.

\end{frame}

\begin{frame}[fragile]\frametitle{Inverse sampling}

<<>>=
u<-runif(1000,min=0,max=1) 

z<-(1/2) * (sqrt(160*u +9)-3)
@

This method can't be used if we can't find the inverse, and it can't be used with multivariate distributions.

\end{frame}

\begin{frame}[fragile]\frametitle{Inverse sampling}

<<echo=FALSE,fig.height=4>>=
hist(z,freq=FALSE)
fn<-function(x){(1/40) * (2*x + 3)}
x<-seq(0,5,by=0.01)
lines(x,fn(x))
@

\end{frame}


\begin{frame}[fragile]\frametitle{Rejection sampling}

If $F^{-1}(u)$ can't be computed, we sample from $f(x)$ as follows:

\begin{enumerate}
\item 
Sample a value $z$ from a distribution $g(z)$ from which sampling is easy, and for which 

\begin{equation}
m g(z) > f(z) \quad m \hbox{ a constant}
\end{equation}

$m g(z)$ is called an envelope function because it envelops $f(z)$.


%\item 
%Compute the ratio

%\begin{equation}
%R = \frac{f(z)}{mg(z)}
%\end{equation}

\item Sample $u\sim Unif(0,1)$. Compute mg(x)*u: this is the maximum value possible of the function mg(x).

\item If $mg(x)u<f(x)$, then $z$ is treated as a draw from $f(x)$. Otherwise return to step 1. 

Note that another way to say this is:

If $u<\frac{f(x)}{mg(x)}$ then accept.

\end{enumerate}

\end{frame}

\begin{frame}[fragile]\frametitle{Rejection sampling}
\begin{enumerate}
\item
For example, consider f(x) as above: 
$f(x) = \frac{1}{40} (2x + 3)$, with $0<x<5$. The maximum height of $f(x)$ is $0.325$. 
\item 
So we need an envelope function that exceeds $0.325$. The uniform density $Unif(0,5)$ has maximum height 0.2, so if we multiply it by 2 we have maximum height $0.4$, which is greater than $0.325$.
\end{enumerate}

\end{frame}

\begin{frame}[fragile]\frametitle{Rejection sampling}


<<echo=FALSE,fig.height=3>>=
fn<-function(x){(1/40) * (2*x + 3)}
x<-seq(0,5,by=0.01)
plot(x,fn(x),type="l",ylim=c(0,.5))
arrows(x0=0,y0=0.4,x1=5,y1=0.4,code=0)
arrows(x0=0,y0=0,x1=0,y1=0.4,code=0)
arrows(x0=5,y0=0,x1=5,y1=0.4,code=0)
@



\end{frame}

\begin{frame}[fragile]\frametitle{Rejection sampling}

In the first step, we sample a number x from a uniform distribution Unif(0,5). This serves to locate a point on the x-axis between 0 and 5 (the domain of $x$).
<<>>=
x<-runif(1,0,5)
@

Keep in mind that even though you can't sample from the function f(x), you can evaluate it for any x:

<<>>=
fn(x)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Rejection sampling}

The next step involves locating a point in the y direction once the x coordinate is fixed. If we draw a number u from Unif(0,1):

<<>>=
u<-runif(1,0,1)
@

then 
$m g(x) u =2*0.2 u$ is a number between $0$ and $2\times 0.2=0.4$:  
<<>>=
0.4*u
0.4*u < fn(x)
@

If this number is less than f(x), that means that the y value falls within f(x), so we accept it, else reject.


\end{frame}

\begin{frame}[fragile]\frametitle{Rejection sampling}

\small
<<>>=
count<-0
k<-1 
accepted<-rep(NA,1000) 
rejected<-rep(NA,1000)
while(k<1001)
{
z<-runif(1,min=0,max=5) 
r<-((1/40)*(2*z+3))/(2*.2)
if(r>runif(1,min=0,max=1)) {
  accepted[k]<-z
  k<-k+1} else {
    rejected[k]<-z
  }
count<-count+1
}
@

\end{frame}

\begin{frame}[fragile]\frametitle{Rejection sampling}

<<rejectionsampling,echo=FALSE,fig.height=4>>=
hist(accepted,freq=F,
     main="Example of rejection sampling")

fn<-function(x){
  (1/40)*(2*x+3)
}

x<-seq(0,5,by=0.01)

lines(x,fn(x))
@



Rejection sampling can be used with multivariate distributions. 

\end{frame}

\begin{frame}[fragile]\frametitle{Some limitations of rejection sampling}

\begin{enumerate}
\item
Finding an envelope function may be difficult.
\item
The acceptance rate would be low if the constant m is set too high and/or if the envelope function is too high relative to f(x), making the algorithm inefficient.   
\end{enumerate}

\end{frame}

\begin{frame}\frametitle{Summary so far}

\begin{enumerate}
\item
We now know what a Markov Chain is.
\item 
We know that there are several methods to sample from a distribution (we saw two).
\end{enumerate}

Next, I will talk about a sampling method using in JAGS (Just Another \textbf{Gibbs Sampler}).

\end{frame}



\section{Gibbs sampling}

\begin{frame}[fragile]\frametitle{Gibbs Sampling}

\textbf{Goal}: sample from a posterior distribution of some parameter(s).

Let $\Theta$ be the vector of parameter values,  say $\Theta=<\theta_1,\theta_2>$. Let $j$ index the $j$-th iteration.

Algorithm:

\begin{enumerate}
\item Assign starting values to $\Theta=<\theta_1,\theta_2>$:

$\Theta^{j=0} \leftarrow S$

\item 
Set $j \leftarrow j + 1$
\item 
\begin{enumerate}
\item[1.] Sample $\theta_1^j \mid \theta_2^{j-1}
$ (e.g., using inversion sampling).
\item[2.] Sample $\theta_2^j \mid \theta_1^{j}$ (e.g., using inversion sampling).
\end{enumerate}
\item
Return to step 1.
\end{enumerate}

Simplifying a bit, eventually, we will start sampling the parameter values from the correct distribution---we will get a sample from the posterior distribution.

Of course, I didn't explain here \textit{why} this works. See Lynch book for details.

\end{frame}

\begin{frame}[fragile]\frametitle{Gibbs sampling: Example}

Sample two random variables from a bivariate normal, where $X\sim N(0,1)$ and $Y\sim N(0,1)$.

\end{frame}

\begin{frame}[fragile]\frametitle{Gibbs sampling: Example}

See MC walk graphic.

\end{frame}

\begin{frame}[fragile]\frametitle{Gibbs sampling: Example}


We can plot the ``trace plot'' of each density, as well as the marginal density: see traceplot graphic.



\end{frame}

\begin{frame}[fragile]\frametitle{Gibbs sampling: Example}


The trace plots show the points that were sampled. The initial values were not realistic, and it could be that the first few iterations do not really yield representative samples. We discard these, and the initial period is called ``burn-in'' (JAGS) or ``warm-up'' (Stan).

The two dimensional trace plot traces the Markov chain walk (see Markov chain walk figure)


\end{frame}


\begin{frame}[fragile]\frametitle{Gibbs sampling: Example}

We can also summarize the marginal distributions. One way is to compute the 95\% credible interval, and the mean or median.

That's what we have been doing in the last few examples we saw.

\end{frame}


\begin{frame}[fragile]\frametitle{Key point regarding MCMC sampling}

\begin{enumerate}
\item
As long as we know the posterior distribution up to proportionality, we can sample from the posterior:

Posterior $\propto$ Lik Prior
\item
There are other sampling methods, such as Metropolis-Hastings, and Hamiltonian MC (Stan uses this; see Stan manual). 
\item
As end-users, we do not need to be able to program these ourselves (except for fun).
\end{enumerate}
\end{frame}

\begin{frame}[fragile]\frametitle{Recall the JAGS commands}
\begin{verbatim}
> headnoun.mod <- jags.model( 
   file="gwmaximal.jag",
   data = headnoun.dat,
   n.chains = 4,
   n.adapt =2000 , quiet=T)
\end{verbatim}
\begin{enumerate}
\item
\texttt{n.chains}: Number of independent chains to run (always have more than 1, I use 4 usually)
\item
\texttt{a.adapt}: the initial period (to be ignored).
\end{enumerate}
\end{frame}

\begin{frame}[fragile]\frametitle{Recall the JAGS commands}

\begin{verbatim}
headnoun.res <- coda.samples(headnoun.mod,
                 var = track.variables,
                 n.iter = 10000,
                 thin = 1)
\end{verbatim}

\begin{enumerate}
\item
\texttt{n.iter}: Number of iterations (walks) in an MCMC chain.
\item
\texttt{thin}: Reduce number of MCMC sequences from posterior (helps in plotting less dense plots). thin=1 means keep all of them. thin=2 means take every alternative one, etc.
\end{enumerate}


\end{frame}



\section{Evaluating the model}

\begin{frame}[fragile]\frametitle{Evaluating the model}

The first thing we want to do is find out whether our model has converged: whether the MCMC sampler is sampling from the posterior distribution.

The function gelman.diag will give you this information, but the traceplots are also informative:

\begin{verbatim}
gelman.diag()
\end{verbatim}

%[See lecture 3, slide 20, for usage]

\end{frame}


\begin{frame}[fragile]\frametitle{Evaluating model fit}

Recall Gibson and Wu data.

 We can examine quality of fit by generating \textbf{predicted} reading times, and comparing them with 
\begin{enumerate}
\item
the actual data 
\item
new data (much better)
\end{enumerate}

An example of posterior predictive checks (using Stan) is given in:

http://www.ling.uni-potsdam.de/$\sim$vasishth/statistics/BayesLMMs.html

\end{frame}

\begin{frame}[fragile]\frametitle{Evaluating model fit}
\framesubtitle{Using the log normal to model the Gibson and Wu data}

The data clearly seem to be log-normally distributed:

\includegraphics[height=6.5cm,width=8cm]{fig_08}



\end{frame}


\begin{frame}[fragile]\frametitle{Evaluating model fit}
\framesubtitle{Using the log normal to model the Gibson and Wu data}

In Stan we simply need to write:

\begin{verbatim}
rt ~ lognormal(mu,sigma)
\end{verbatim}


\end{frame}


\begin{frame}[fragile]\frametitle{Evaluating model fit}
\framesubtitle{Using new data from Vasishth et al 2013}
\small
These extreme values turn up again in the subject relative condition. 
Either we could fit a mixture of models to the data, or we transform the data to stabilize variance.
\normalsize
\includegraphics[height=6.5cm,width=8cm]{fig_05}
\end{frame}

\section{Model assumptions}

\begin{frame}[fragile]\frametitle{Checking model assumptions}

<<echo=FALSE>>=
data<-read.table("data/gibsonwu2012data.txt",header=T)
data$rrt<- -1000/data$rt
data$x <- ifelse(
  data$type%in%c("subj-ext"),-0.5,0.5)
headnoun<-subset(data,region=="headnoun")
library(lme4)
@

The best model in the Gibson and Wu data-set is:

<<>>=
m0<-lmer(rrt~x+(1|subj)+(1|item),headnoun)
### varying slopes for subj
m1<-lmer(rrt~x+(1+x|subj)+(1|item),headnoun)
##
m1a<-lmer(rrt~x+(x||subj)+(1|item),headnoun)
### varying intercepts and slopes for subj and item
m2<-lmer(rrt~x+(1+x|subj)+(1+x|item),headnoun)
m2a<-lmer(rrt~x+(x||subj)+(x||item),headnoun)
@

\end{frame}

\begin{frame}[fragile]\frametitle{Checking model assumptions}

\tiny
<<>>=
anova(m0,m1,m1a,m2,m2a)
@


\end{frame}

\begin{frame}[fragile]\frametitle{Checking model assumptions}

<<fig.height=4>>=
library(car)
qqPlot(residuals(m0))
@

\end{frame}

\section{Consequences of ignoring non-normality assumption}

\begin{frame}[fragile]\frametitle{A bad consequence of ignoring model assumptions}
\small
The published result in Gibson and Wu is driven by 8 out of 547 extreme values $> 3000 ms$:

\begin{verbatim}
Fixed effects:
            Estimate Std. Error t value
(Intercept)   548.43      51.56  10.638
x            -120.39      48.01  -2.508
\end{verbatim}

Normalizing the residuals leads to no effect at all:

\begin{verbatim}
            Estimate Std. Error t value
(Intercept) -2.67217    0.13758 -19.423
x           -0.07494    0.08172  -0.917
\end{verbatim}

Removing the 8/547 extreme values in raw RT:

\begin{verbatim}
            Estimate Std. Error t value
(Intercept)   494.59      34.09  14.509
x             -12.66      29.94  -0.423
\end{verbatim}

\end{frame}


\begin{frame}[fragile]\frametitle{Gibson and Wu: the published result}

<<>>=
head.means<-aggregate(rt~subj+type,
                      mean,data=headnoun)
@

Now, after aggregation, we can run a paired t-test:

<<>>=
t_test_result<-t.test(subset(head.means,
                             type=="subj-ext")$rt,
       subset(head.means,
              type=="obj-ext")$rt,paired=T)
t_test_result$statistic;t_test_result$p.value
@

The difference was statistically significant. 
%(They did an ANOVA, but it's the same thing, as $t^2=F$.)

\end{frame}

\begin{frame}\frametitle{The lesson in this example}
The lesson here is to 
\begin{enumerate}
\item first examine your data graphically

\item check whether model assumptions are satisfied 
\end{enumerate}

Fitting models is not primarily about checking for statistical significance, we are also 

\begin{enumerate}
\item 
defining how we think the data were generated
\item
generating predictions about what future data will look like.
\end{enumerate}

\end{frame}



\begin{frame}[fragile]\frametitle{Non-normality of residuals can lead to loss of power}

I showed an example where extreme values (which lead to non-normality) lead to an over-enthusiastic p-value.

Here is another consequence:

<<>>=
## number of simulations:
nsim<-100
## sample size:
n<-100
## predictor:
pred<-rep(c(0,1),each=n/2)
## matrices for storing results:
store<-matrix(0,nsim,4)
storenorm<-matrix(0,nsim,4)
## true effect:
beta.1<-0.5
@

\end{frame}

\begin{frame}[fragile]\frametitle{Non-normality of residuals can lead to loss of power}

<<>>=
for(i in 1:nsim){
errors<-rlnorm(n)
errorsnorm<-rnorm(n)
y<-5 + beta.1*pred + errors ## generate data:
ynorm<-5 + beta.1*pred + errorsnorm
fm<-lm(y~pred)
fmnorm<-lm(ynorm~pred)
## store coef., SE, t-value, p-value:
store[i,1:4] <- summary(fm)$coef[2,1:4]
storenorm[i,1:4] <- summary(fmnorm)$coef[2,1:4]
}
@

\end{frame}

\begin{frame}[fragile]\frametitle{Non-normality of residuals can lead to loss of power}

<<>>=
## ``observed'' power for raw scores:
mean(store[,4]<0.05)
mean(storenorm[,4]<0.05)
@

%Exercise: increase sample size to n=1000 and see what changes.

<<powerdifferential,echo=FALSE>>=
size<-seq(100,1000,by=10)
powerdiff<-rep(NA,length(size))
nsim<-100
for(j in 1:length(size)){
## sample size:
n<-size[j]
## predictor:
pred<-rep(c(0,1),each=n/2)
## matrices for storing results:
store<-matrix(0,nsim,4)
storenorm<-matrix(0,nsim,4)
## true effect:
beta.1<-0.5

for(i in 1:nsim){
errors<-rlnorm(n)
errorsnorm<-rnorm(n)
#errors<-errors-mean(errors)
#errorsnorm<-errorsnorm-mean(errorsnorm)
y<-5 + beta.1*pred + errors ## generate data:
ynorm<-5 + beta.1*pred + errorsnorm
fm<-lm(y~pred)
fmnorm<-lm(ynorm~pred)
## store coef., SE, t-value, p-value:
store[i,1:4] <- summary(fm)$coef[2,1:4]
storenorm[i,1:4] <- summary(fmnorm)$coef[2,1:4]
}
powerdiff[j]<-mean(storenorm[,4]<0.05)-mean(store[,4]<0.05)
}
@
\end{frame}

\begin{frame}[fragile]\frametitle{The cost of non-normality: loss of power}
<<echo=FALSE,fig.height=5>>=
plot(size,powerdiff,xlab="sample size",
     ylab="loss of power",
     main="why violating normality is bad for null results")
@

\end{frame}

\begin{frame}[fragile]\frametitle{Non-normality of residuals can lead to loss of power}

<<fig.height=4>>=
library(car)
op<-par(mfrow=c(1,2),pty="s")
qqPlot(residuals(fm))
qqPlot(residuals(fmnorm))
@


\end{frame}

\section{Using the Box-Cox method}

\begin{frame}[fragile]\frametitle[fragile]{What transform to use?}

Box-Cox method to find the appropriate variance stabilizing transform:

<<fig.height=4>>=
library(MASS)
boxcox(rt~type*subj,data=headnoun)
@

\end{frame}

\begin{frame}[fragile]\frametitle[fragile]{What transform to use?}


\begin{enumerate}
\item 
A $\lambda$ of -1 implies a reciprocal transform.
\item A $\lambda$ of 0 implies a log transform.
\end{enumerate}

We generally use the reciprocal or log transform for reading time data.

See Venables and Ripley 2002 for details (or Box and Cox, 1964), or read the Linear Modeling lecture notes.

\end{frame}




\section{Concluding remarks}

\begin{frame}\frametitle{Concluding remarks}

\begin{enumerate}
\item We now have some idea about how we can sample from a posterior distribution.
\item We also have some idea of how to check for convergence, evaluate model assumptions using posterior predictive checks.
\end{enumerate}

In the next lecture, we will fit some more complex linear mixed models. 

\end{frame}

\end{document}

